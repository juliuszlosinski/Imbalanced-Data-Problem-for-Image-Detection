{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 64\n",
    "latent_dimension = 100\n",
    "number_of_epochs = 200\n",
    "learning_rate = 0.002\n",
    "beta_01 = 0.5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=image_size),\n",
    "    transforms.CenterCrop(size=image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # Normalizing to [-1, 1]\n",
    "])\n",
    "dataset_path = \"./dataset/training-GAN/Test/\"\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # First layer\n",
    "        self.conv_transpose_2d_01 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=latent_dimension, out_channels=512, kernel_size=4,\n",
    "            stride=1, padding=0, bias=False\n",
    "        )\n",
    "        self.batch_norm_2d_01 = torch.nn.BatchNorm2d(num_features=512)\n",
    "        self.relu_01 = torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Second layer\n",
    "        self.conv_transpose_2d_02 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=512, out_channels=256, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.batch_norm_2d_02 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.relu_02 = torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Third layer\n",
    "        self.conv_transpose_2d_03 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=256, out_channels=128, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.batch_norm_2d_03 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.relu_03 = torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Fourth layer\n",
    "        self.conv_transpose_2d_04 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=128, out_channels=64, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.batch_norm_2d_04 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.relu_04 = torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Fifth layer\n",
    "        self.conv_transpose_2d_05 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=64, out_channels=3, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_transpose_2d_01(x)\n",
    "        x = self.batch_norm_2d_01(x)\n",
    "        x = self.relu_01(x)\n",
    "        \n",
    "        x = self.conv_transpose_2d_02(x)\n",
    "        x = self.batch_norm_2d_02(x)\n",
    "        x = self.relu_02(x)\n",
    "        \n",
    "        x = self.conv_transpose_2d_03(x)\n",
    "        x = self.batch_norm_2d_03(x)\n",
    "        x = self.relu_03(x)\n",
    "        \n",
    "        x = self.conv_transpose_2d_04(x)\n",
    "        x = self.batch_norm_2d_04(x)\n",
    "        x = self.relu_04(x)\n",
    "        \n",
    "        x = self.conv_transpose_2d_05(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # First layer\n",
    "        self.conv2d_01 = torch.nn.Conv2d(\n",
    "            in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.leaky_relu_01 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        \n",
    "        # Second layer\n",
    "        self.conv2d_02 = torch.nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.batch_norm_2d_02 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.leaky_relu_02 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        \n",
    "        # Third layer\n",
    "        self.conv2d_03 = torch.nn.Conv2d(\n",
    "            in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.batch_norm_2d_03 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.leaky_relu_03 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        \n",
    "        # Fourth layer\n",
    "        self.conv2d_04 = torch.nn.Conv2d(\n",
    "            in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.batch_norm_2d_04 = torch.nn.BatchNorm2d(num_features=512)\n",
    "        self.leaky_relu_04 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        \n",
    "        # Fivth layer\n",
    "        self.conv2d_05 = torch.nn.Conv2d(\n",
    "            in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False\n",
    "        )\n",
    "        self.sigmoid_05 = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_01(x)\n",
    "        x = self.leaky_relu_01(x)\n",
    "        \n",
    "        x = self.conv2d_02(x)\n",
    "        x = self.batch_norm_2d_02(x)\n",
    "        x = self.leaky_relu_02(x)\n",
    "        \n",
    "        x = self.conv2d_03(x)\n",
    "        x = self.batch_norm_2d_03(x)\n",
    "        x = self.leaky_relu_03(x)\n",
    "        \n",
    "        x = self.conv2d_04(x)\n",
    "        x = self.batch_norm_2d_04(x)\n",
    "        x = self.leaky_relu_04(x)\n",
    "        \n",
    "        x = self.conv2d_05(x)\n",
    "        x = self.sigmoid_05(x)\n",
    "        \n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: Generator(\n",
      "  (conv_transpose_2d_01): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (batch_norm_2d_01): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_01): ReLU(inplace=True)\n",
      "  (conv_transpose_2d_02): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm_2d_02): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_02): ReLU(inplace=True)\n",
      "  (conv_transpose_2d_03): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm_2d_03): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_03): ReLU(inplace=True)\n",
      "  (conv_transpose_2d_04): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm_2d_04): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_04): ReLU(inplace=True)\n",
      "  (conv_transpose_2d_05): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "\n",
      "Discriminator: Discriminator(\n",
      "  (conv2d_01): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (leaky_relu_01): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (conv2d_02): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm_2d_02): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky_relu_02): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (conv2d_03): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm_2d_03): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky_relu_03): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (conv2d_04): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm_2d_04): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky_relu_04): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (conv2d_05): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (sigmoid_05): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "generator_model = Generator(latent_dimension=latent_dimension).to(device)\n",
    "print(f\"Generator: {generator_model}\\n\")\n",
    "\n",
    "discriminator_model = Discriminator().to(device)\n",
    "print(f\"Discriminator: {discriminator_model}\")\n",
    "\n",
    "criterion = torch.nn.BCELoss() # Binary cross entropy loss/ BCE\n",
    "optimizer_generator = torch.optim.Adam(generator_model.parameters(), lr=learning_rate, betas=(beta_01, 0.999))\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator_model.parameters(), lr=learning_rate, betas=(beta_01, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200] Batch 0/1 Loss D: 6.2317, Loss G: 18.8154\n",
      "Epoch [1/200] Batch 0/1 Loss D: 8.3506, Loss G: 10.3048\n",
      "Epoch [2/200] Batch 0/1 Loss D: 0.8120, Loss G: 5.8034\n",
      "Epoch [3/200] Batch 0/1 Loss D: 1.1487, Loss G: 3.5989\n",
      "Epoch [4/200] Batch 0/1 Loss D: 2.1630, Loss G: 3.5016\n",
      "Epoch [5/200] Batch 0/1 Loss D: 1.5439, Loss G: 7.0314\n",
      "Epoch [6/200] Batch 0/1 Loss D: 3.1593, Loss G: 1.5379\n",
      "Epoch [7/200] Batch 0/1 Loss D: 1.3801, Loss G: 1.7186\n",
      "Epoch [8/200] Batch 0/1 Loss D: 1.2217, Loss G: 4.3166\n",
      "Epoch [9/200] Batch 0/1 Loss D: 1.7265, Loss G: 1.8303\n",
      "Epoch [10/200] Batch 0/1 Loss D: 1.3054, Loss G: 4.3833\n",
      "Epoch [11/200] Batch 0/1 Loss D: 0.9011, Loss G: 2.7761\n",
      "Epoch [12/200] Batch 0/1 Loss D: 1.0390, Loss G: 4.5660\n",
      "Epoch [13/200] Batch 0/1 Loss D: 0.9102, Loss G: 2.3344\n",
      "Epoch [14/200] Batch 0/1 Loss D: 2.2780, Loss G: 6.7019\n",
      "Epoch [15/200] Batch 0/1 Loss D: 3.3561, Loss G: 2.0557\n",
      "Epoch [16/200] Batch 0/1 Loss D: 1.2099, Loss G: 2.0518\n",
      "Epoch [17/200] Batch 0/1 Loss D: 1.4635, Loss G: 5.1419\n",
      "Epoch [18/200] Batch 0/1 Loss D: 2.8172, Loss G: 1.2159\n",
      "Epoch [19/200] Batch 0/1 Loss D: 2.4463, Loss G: 5.3996\n",
      "Epoch [20/200] Batch 0/1 Loss D: 2.1639, Loss G: 1.4821\n",
      "Epoch [21/200] Batch 0/1 Loss D: 1.4291, Loss G: 2.8091\n",
      "Epoch [22/200] Batch 0/1 Loss D: 1.0246, Loss G: 1.7010\n",
      "Epoch [23/200] Batch 0/1 Loss D: 1.2922, Loss G: 5.7495\n",
      "Epoch [24/200] Batch 0/1 Loss D: 2.7825, Loss G: 0.6555\n",
      "Epoch [25/200] Batch 0/1 Loss D: 2.4043, Loss G: 4.6092\n",
      "Epoch [26/200] Batch 0/1 Loss D: 1.4842, Loss G: 2.2528\n",
      "Epoch [27/200] Batch 0/1 Loss D: 0.6977, Loss G: 2.3988\n",
      "Epoch [28/200] Batch 0/1 Loss D: 0.6135, Loss G: 3.9824\n",
      "Epoch [29/200] Batch 0/1 Loss D: 0.6136, Loss G: 3.1375\n",
      "Epoch [30/200] Batch 0/1 Loss D: 1.9066, Loss G: 7.0136\n",
      "Epoch [31/200] Batch 0/1 Loss D: 4.0079, Loss G: 2.0120\n",
      "Epoch [32/200] Batch 0/1 Loss D: 1.0189, Loss G: 0.6361\n",
      "Epoch [33/200] Batch 0/1 Loss D: 1.5629, Loss G: 5.3849\n",
      "Epoch [34/200] Batch 0/1 Loss D: 1.6659, Loss G: 3.0196\n",
      "Epoch [35/200] Batch 0/1 Loss D: 0.5446, Loss G: 2.3066\n",
      "Epoch [36/200] Batch 0/1 Loss D: 0.3484, Loss G: 3.8785\n",
      "Epoch [37/200] Batch 0/1 Loss D: 0.2823, Loss G: 4.4161\n",
      "Epoch [38/200] Batch 0/1 Loss D: 0.3725, Loss G: 4.6027\n",
      "Epoch [39/200] Batch 0/1 Loss D: 0.4676, Loss G: 5.3258\n",
      "Epoch [40/200] Batch 0/1 Loss D: 0.5411, Loss G: 4.2454\n",
      "Epoch [41/200] Batch 0/1 Loss D: 1.4255, Loss G: 8.0225\n",
      "Epoch [42/200] Batch 0/1 Loss D: 2.5175, Loss G: 3.4632\n",
      "Epoch [43/200] Batch 0/1 Loss D: 0.4499, Loss G: 1.9104\n",
      "Epoch [44/200] Batch 0/1 Loss D: 1.2373, Loss G: 4.8909\n",
      "Epoch [45/200] Batch 0/1 Loss D: 0.8898, Loss G: 4.0818\n",
      "Epoch [46/200] Batch 0/1 Loss D: 0.8109, Loss G: 4.4097\n",
      "Epoch [47/200] Batch 0/1 Loss D: 0.8950, Loss G: 3.5983\n",
      "Epoch [48/200] Batch 0/1 Loss D: 0.8385, Loss G: 4.4160\n",
      "Epoch [49/200] Batch 0/1 Loss D: 0.6090, Loss G: 3.8034\n",
      "Epoch [50/200] Batch 0/1 Loss D: 0.7036, Loss G: 3.0195\n",
      "Epoch [51/200] Batch 0/1 Loss D: 0.5925, Loss G: 4.3712\n",
      "Epoch [52/200] Batch 0/1 Loss D: 0.7665, Loss G: 3.0445\n",
      "Epoch [53/200] Batch 0/1 Loss D: 0.5335, Loss G: 4.4853\n",
      "Epoch [54/200] Batch 0/1 Loss D: 0.5135, Loss G: 3.5686\n",
      "Epoch [55/200] Batch 0/1 Loss D: 0.7258, Loss G: 5.0735\n",
      "Epoch [56/200] Batch 0/1 Loss D: 0.6408, Loss G: 1.8698\n",
      "Epoch [57/200] Batch 0/1 Loss D: 1.8964, Loss G: 7.5054\n",
      "Epoch [58/200] Batch 0/1 Loss D: 2.0365, Loss G: 3.8678\n",
      "Epoch [59/200] Batch 0/1 Loss D: 1.5870, Loss G: 4.3152\n",
      "Epoch [60/200] Batch 0/1 Loss D: 0.9176, Loss G: 2.1356\n",
      "Epoch [61/200] Batch 0/1 Loss D: 0.7860, Loss G: 3.0447\n",
      "Epoch [62/200] Batch 0/1 Loss D: 0.7181, Loss G: 1.9890\n",
      "Epoch [63/200] Batch 0/1 Loss D: 0.8757, Loss G: 4.5449\n",
      "Epoch [64/200] Batch 0/1 Loss D: 1.2736, Loss G: 2.3035\n",
      "Epoch [65/200] Batch 0/1 Loss D: 1.3023, Loss G: 3.4482\n",
      "Epoch [66/200] Batch 0/1 Loss D: 1.8614, Loss G: 2.2449\n",
      "Epoch [67/200] Batch 0/1 Loss D: 2.0812, Loss G: 6.8633\n",
      "Epoch [68/200] Batch 0/1 Loss D: 2.6028, Loss G: 1.6754\n",
      "Epoch [69/200] Batch 0/1 Loss D: 1.4231, Loss G: 2.1266\n",
      "Epoch [70/200] Batch 0/1 Loss D: 0.8892, Loss G: 2.8292\n",
      "Epoch [71/200] Batch 0/1 Loss D: 1.0005, Loss G: 2.2951\n",
      "Epoch [72/200] Batch 0/1 Loss D: 1.2300, Loss G: 4.5901\n",
      "Epoch [73/200] Batch 0/1 Loss D: 1.5533, Loss G: 1.2170\n",
      "Epoch [74/200] Batch 0/1 Loss D: 1.3482, Loss G: 2.5504\n",
      "Epoch [75/200] Batch 0/1 Loss D: 1.0797, Loss G: 1.4561\n",
      "Epoch [76/200] Batch 0/1 Loss D: 0.8936, Loss G: 2.2821\n",
      "Epoch [77/200] Batch 0/1 Loss D: 0.8724, Loss G: 3.4682\n",
      "Epoch [78/200] Batch 0/1 Loss D: 0.8713, Loss G: 1.3436\n",
      "Epoch [79/200] Batch 0/1 Loss D: 1.0875, Loss G: 3.0014\n",
      "Epoch [80/200] Batch 0/1 Loss D: 1.1964, Loss G: 1.5520\n",
      "Epoch [81/200] Batch 0/1 Loss D: 1.1560, Loss G: 1.9173\n",
      "Epoch [82/200] Batch 0/1 Loss D: 0.9714, Loss G: 2.0853\n",
      "Epoch [83/200] Batch 0/1 Loss D: 1.0691, Loss G: 2.0688\n",
      "Epoch [84/200] Batch 0/1 Loss D: 0.9511, Loss G: 1.5895\n",
      "Epoch [85/200] Batch 0/1 Loss D: 0.9930, Loss G: 3.1209\n",
      "Epoch [86/200] Batch 0/1 Loss D: 1.0587, Loss G: 1.0981\n",
      "Epoch [87/200] Batch 0/1 Loss D: 1.3086, Loss G: 4.6957\n",
      "Epoch [88/200] Batch 0/1 Loss D: 1.7185, Loss G: 0.8044\n",
      "Epoch [89/200] Batch 0/1 Loss D: 2.6331, Loss G: 5.3669\n",
      "Epoch [90/200] Batch 0/1 Loss D: 2.4609, Loss G: 1.5822\n",
      "Epoch [91/200] Batch 0/1 Loss D: 1.1099, Loss G: 1.9956\n",
      "Epoch [92/200] Batch 0/1 Loss D: 1.5260, Loss G: 2.0166\n",
      "Epoch [93/200] Batch 0/1 Loss D: 1.3066, Loss G: 1.9110\n",
      "Epoch [94/200] Batch 0/1 Loss D: 1.0094, Loss G: 1.6663\n",
      "Epoch [95/200] Batch 0/1 Loss D: 1.5150, Loss G: 2.5198\n",
      "Epoch [96/200] Batch 0/1 Loss D: 1.1912, Loss G: 2.3648\n",
      "Epoch [97/200] Batch 0/1 Loss D: 1.3081, Loss G: 1.7827\n",
      "Epoch [98/200] Batch 0/1 Loss D: 1.1024, Loss G: 1.9602\n",
      "Epoch [99/200] Batch 0/1 Loss D: 1.0236, Loss G: 1.4661\n",
      "Epoch [100/200] Batch 0/1 Loss D: 1.2819, Loss G: 2.3354\n",
      "Epoch [101/200] Batch 0/1 Loss D: 1.1908, Loss G: 2.0224\n",
      "Epoch [102/200] Batch 0/1 Loss D: 0.9432, Loss G: 2.1779\n",
      "Epoch [103/200] Batch 0/1 Loss D: 1.0743, Loss G: 2.8680\n",
      "Epoch [104/200] Batch 0/1 Loss D: 1.1640, Loss G: 1.7552\n",
      "Epoch [105/200] Batch 0/1 Loss D: 1.0142, Loss G: 2.4926\n",
      "Epoch [106/200] Batch 0/1 Loss D: 0.7023, Loss G: 2.4650\n",
      "Epoch [107/200] Batch 0/1 Loss D: 0.7990, Loss G: 2.1397\n",
      "Epoch [108/200] Batch 0/1 Loss D: 0.9730, Loss G: 2.0478\n",
      "Epoch [109/200] Batch 0/1 Loss D: 1.3823, Loss G: 2.7921\n",
      "Epoch [110/200] Batch 0/1 Loss D: 1.3208, Loss G: 2.1949\n",
      "Epoch [111/200] Batch 0/1 Loss D: 0.9655, Loss G: 1.8628\n",
      "Epoch [112/200] Batch 0/1 Loss D: 0.8612, Loss G: 4.8343\n",
      "Epoch [113/200] Batch 0/1 Loss D: 1.3974, Loss G: 0.7922\n",
      "Epoch [114/200] Batch 0/1 Loss D: 1.4394, Loss G: 3.0789\n",
      "Epoch [115/200] Batch 0/1 Loss D: 0.9420, Loss G: 1.8382\n",
      "Epoch [116/200] Batch 0/1 Loss D: 1.0864, Loss G: 3.0140\n",
      "Epoch [117/200] Batch 0/1 Loss D: 1.0946, Loss G: 1.8457\n",
      "Epoch [118/200] Batch 0/1 Loss D: 0.9926, Loss G: 2.5539\n",
      "Epoch [119/200] Batch 0/1 Loss D: 1.0704, Loss G: 2.1641\n",
      "Epoch [120/200] Batch 0/1 Loss D: 0.8938, Loss G: 2.2747\n",
      "Epoch [121/200] Batch 0/1 Loss D: 0.8269, Loss G: 1.9209\n",
      "Epoch [122/200] Batch 0/1 Loss D: 0.6611, Loss G: 2.6598\n",
      "Epoch [123/200] Batch 0/1 Loss D: 0.8290, Loss G: 3.3453\n",
      "Epoch [124/200] Batch 0/1 Loss D: 0.7591, Loss G: 0.8068\n",
      "Epoch [125/200] Batch 0/1 Loss D: 1.4086, Loss G: 4.0683\n",
      "Epoch [126/200] Batch 0/1 Loss D: 1.1938, Loss G: 1.4792\n",
      "Epoch [127/200] Batch 0/1 Loss D: 0.7799, Loss G: 2.4859\n",
      "Epoch [128/200] Batch 0/1 Loss D: 0.8066, Loss G: 2.1721\n",
      "Epoch [129/200] Batch 0/1 Loss D: 1.0459, Loss G: 3.4402\n",
      "Epoch [130/200] Batch 0/1 Loss D: 0.9923, Loss G: 1.6249\n",
      "Epoch [131/200] Batch 0/1 Loss D: 1.0950, Loss G: 4.1114\n",
      "Epoch [132/200] Batch 0/1 Loss D: 0.9932, Loss G: 1.5495\n",
      "Epoch [133/200] Batch 0/1 Loss D: 0.9376, Loss G: 3.8908\n",
      "Epoch [134/200] Batch 0/1 Loss D: 1.0391, Loss G: 2.3592\n",
      "Epoch [135/200] Batch 0/1 Loss D: 1.0420, Loss G: 3.7650\n",
      "Epoch [136/200] Batch 0/1 Loss D: 1.1841, Loss G: 1.0044\n",
      "Epoch [137/200] Batch 0/1 Loss D: 1.4528, Loss G: 5.2065\n",
      "Epoch [138/200] Batch 0/1 Loss D: 1.9807, Loss G: 1.5018\n",
      "Epoch [139/200] Batch 0/1 Loss D: 0.7916, Loss G: 2.3523\n",
      "Epoch [140/200] Batch 0/1 Loss D: 0.7222, Loss G: 2.5470\n",
      "Epoch [141/200] Batch 0/1 Loss D: 0.8482, Loss G: 1.7240\n",
      "Epoch [142/200] Batch 0/1 Loss D: 0.8401, Loss G: 3.0573\n",
      "Epoch [143/200] Batch 0/1 Loss D: 0.8377, Loss G: 1.6467\n",
      "Epoch [144/200] Batch 0/1 Loss D: 0.8620, Loss G: 3.6255\n",
      "Epoch [145/200] Batch 0/1 Loss D: 1.0383, Loss G: 4.0263\n",
      "Epoch [146/200] Batch 0/1 Loss D: 1.2809, Loss G: 1.4402\n",
      "Epoch [147/200] Batch 0/1 Loss D: 1.2752, Loss G: 4.2322\n",
      "Epoch [148/200] Batch 0/1 Loss D: 1.3409, Loss G: 1.9686\n",
      "Epoch [149/200] Batch 0/1 Loss D: 0.6685, Loss G: 2.0898\n",
      "Epoch [150/200] Batch 0/1 Loss D: 0.5656, Loss G: 2.9699\n",
      "Epoch [151/200] Batch 0/1 Loss D: 0.6144, Loss G: 2.4336\n",
      "Epoch [152/200] Batch 0/1 Loss D: 0.7061, Loss G: 2.9500\n",
      "Epoch [153/200] Batch 0/1 Loss D: 0.6631, Loss G: 2.7171\n",
      "Epoch [154/200] Batch 0/1 Loss D: 0.5677, Loss G: 3.5526\n",
      "Epoch [155/200] Batch 0/1 Loss D: 0.4484, Loss G: 2.9400\n",
      "Epoch [156/200] Batch 0/1 Loss D: 0.5073, Loss G: 5.6290\n",
      "Epoch [157/200] Batch 0/1 Loss D: 0.8338, Loss G: 1.2546\n",
      "Epoch [158/200] Batch 0/1 Loss D: 1.1388, Loss G: 5.8906\n",
      "Epoch [159/200] Batch 0/1 Loss D: 1.2995, Loss G: 2.1275\n",
      "Epoch [160/200] Batch 0/1 Loss D: 0.7114, Loss G: 3.3233\n",
      "Epoch [161/200] Batch 0/1 Loss D: 0.6349, Loss G: 2.6581\n",
      "Epoch [162/200] Batch 0/1 Loss D: 0.6784, Loss G: 2.7022\n",
      "Epoch [163/200] Batch 0/1 Loss D: 0.6782, Loss G: 2.0236\n",
      "Epoch [164/200] Batch 0/1 Loss D: 0.8409, Loss G: 4.0064\n",
      "Epoch [165/200] Batch 0/1 Loss D: 1.4185, Loss G: 1.5063\n",
      "Epoch [166/200] Batch 0/1 Loss D: 1.4198, Loss G: 3.1359\n",
      "Epoch [167/200] Batch 0/1 Loss D: 0.6887, Loss G: 2.8949\n",
      "Epoch [168/200] Batch 0/1 Loss D: 0.5784, Loss G: 1.9412\n",
      "Epoch [169/200] Batch 0/1 Loss D: 0.7765, Loss G: 5.2953\n",
      "Epoch [170/200] Batch 0/1 Loss D: 1.4625, Loss G: 1.0555\n",
      "Epoch [171/200] Batch 0/1 Loss D: 1.6096, Loss G: 4.8184\n",
      "Epoch [172/200] Batch 0/1 Loss D: 1.8999, Loss G: 1.6625\n",
      "Epoch [173/200] Batch 0/1 Loss D: 1.3393, Loss G: 1.6020\n",
      "Epoch [174/200] Batch 0/1 Loss D: 1.0002, Loss G: 3.2086\n",
      "Epoch [175/200] Batch 0/1 Loss D: 1.3412, Loss G: 1.8322\n",
      "Epoch [176/200] Batch 0/1 Loss D: 0.9623, Loss G: 1.6262\n",
      "Epoch [177/200] Batch 0/1 Loss D: 0.9391, Loss G: 2.9494\n",
      "Epoch [178/200] Batch 0/1 Loss D: 0.7158, Loss G: 2.7663\n",
      "Epoch [179/200] Batch 0/1 Loss D: 0.8014, Loss G: 3.1834\n",
      "Epoch [180/200] Batch 0/1 Loss D: 0.5990, Loss G: 2.4535\n",
      "Epoch [181/200] Batch 0/1 Loss D: 0.6943, Loss G: 4.0062\n",
      "Epoch [182/200] Batch 0/1 Loss D: 0.8050, Loss G: 3.1402\n",
      "Epoch [183/200] Batch 0/1 Loss D: 0.9426, Loss G: 5.7688\n",
      "Epoch [184/200] Batch 0/1 Loss D: 1.4836, Loss G: 1.1313\n",
      "Epoch [185/200] Batch 0/1 Loss D: 1.7089, Loss G: 6.1470\n",
      "Epoch [186/200] Batch 0/1 Loss D: 2.4195, Loss G: 0.9832\n",
      "Epoch [187/200] Batch 0/1 Loss D: 1.6283, Loss G: 2.9336\n",
      "Epoch [188/200] Batch 0/1 Loss D: 1.1304, Loss G: 1.8171\n",
      "Epoch [189/200] Batch 0/1 Loss D: 1.0054, Loss G: 2.5340\n",
      "Epoch [190/200] Batch 0/1 Loss D: 0.7496, Loss G: 2.3247\n",
      "Epoch [191/200] Batch 0/1 Loss D: 0.7883, Loss G: 1.8135\n",
      "Epoch [192/200] Batch 0/1 Loss D: 0.8642, Loss G: 2.6554\n",
      "Epoch [193/200] Batch 0/1 Loss D: 1.0314, Loss G: 2.9697\n",
      "Epoch [194/200] Batch 0/1 Loss D: 0.9442, Loss G: 2.1762\n",
      "Epoch [195/200] Batch 0/1 Loss D: 0.6549, Loss G: 4.9796\n",
      "Epoch [196/200] Batch 0/1 Loss D: 1.0240, Loss G: 1.1596\n",
      "Epoch [197/200] Batch 0/1 Loss D: 1.0993, Loss G: 4.8040\n",
      "Epoch [198/200] Batch 0/1 Loss D: 1.3683, Loss G: 1.9263\n",
      "Epoch [199/200] Batch 0/1 Loss D: 0.8252, Loss G: 3.2651\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "fixed_noise = torch.randn(64, latent_dimension, 1, 1, device=device)\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    for i, (data, _) in enumerate(data_loader):\n",
    "        \n",
    "        # Updating discriminator.\n",
    "        discriminator_model.zero_grad()  # Zeroing/clearing gradients.\n",
    "        real_data = data.to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "        label_real = torch.ones(batch_size, device=device)  # [1, 1, ..., batch_size]\n",
    "        label_fake = torch.zeros(batch_size, device=device)  # [0, 0, ..., batch_size]\n",
    "        \n",
    "        # Discriminator loss on real data\n",
    "        output_real = discriminator_model(real_data)  # y predicted\n",
    "        loss_real = criterion(output_real, label_real)  # BCE - Binary Cross Entropy\n",
    "        \n",
    "        # Discriminator loss on fake data\n",
    "        noise = torch.randn(batch_size, latent_dimension, 1, 1, device=device)\n",
    "        fake_data = generator_model(noise)\n",
    "        output_fake = discriminator_model(fake_data.detach())  # Detach to avoid backprop through generator\n",
    "        loss_fake = criterion(output_fake, label_fake)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        loss_discriminator = loss_real + loss_fake\n",
    "        loss_discriminator.backward()  # Calculating gradients.\n",
    "        optimizer_discriminator.step()  # Updating the weights by using gradients.\n",
    "        \n",
    "        # Updating generator.\n",
    "        generator_model.zero_grad()  # Zeroing/clearing gradients.\n",
    "        label_gen = torch.ones(batch_size, device=device)  # Trick discriminator into believing fakes are real\n",
    "        output_fake = discriminator_model(fake_data)\n",
    "        loss_generator = criterion(output_fake, label_gen)\n",
    "        loss_generator.backward()  # Calculating and propagating gradients.\n",
    "        optimizer_generator.step()\n",
    "        \n",
    "        # Logging progress\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch [{epoch}/{number_of_epochs}] Batch {i}/{len(data_loader)} \"\n",
    "                  f\"Loss D: {loss_discriminator:.4f}, Loss G: {loss_generator:.4f}\")\n",
    "    \n",
    "    # Save generated images after each epoch\n",
    "    with torch.no_grad():\n",
    "        fake_images = generator_model(fixed_noise).detach().cpu()\n",
    "        utils.save_image(fake_images, f\"output_epoch_{epoch}.png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model.eval() # Activating evaluation mode to deactive dropout or batch normalization updates.\n",
    "noise = torch.randn(1, latent_dimension, 1, 1, device=device) # Single image (batch size = 1)\n",
    "\n",
    "with torch.no_grad(): # No gradient calculation is necessary/needed\n",
    "    fake_image = generator_model(noise).detach().cpu()\n",
    "    \n",
    "utils.save_image(fake_image, \"generated_image.png\", normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
